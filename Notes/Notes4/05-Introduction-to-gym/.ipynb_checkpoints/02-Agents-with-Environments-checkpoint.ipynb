{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents with Environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create our first agent which interacts with the environment based on the observations and not just in a random manner.<br />\n",
    "In fact we will try to create an agent for the Mountain car environment https://gym.openai.com/envs/MountainCar-v0/, where the goal is to reach the top of the mountain (and stop there), but the engine is not strong enough to reach it in a single pass.<br />\n",
    "Thus we need to find a strategy how to drive back and forth within the valley to gain momentum to be finally able to reach the top\n",
    "\n",
    "We start by importing the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to slow down the game a little bit\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again create the environment in the same way as shown in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"MountainCar-v0\"  # Use the exact same name as stated on gym.openai\n",
    "env = gym.make(env_name)  # use gym.make to create your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to create such an agent we first need to understand what information we get from the environment and what actions are possible.\n",
    "You can get this information by checking the first few lines of the corresponding source code:https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "\n",
    " 1. observation: The observation is a list containing the two entries:\n",
    "     1. position (x-coordinate of the car)\n",
    "     2. velocity (speed of the car, either forward or backward (positive or negative)\n",
    " 2. actions: The following actions are possible within this environment\n",
    "     1. Accelerate to the left (or in other words use the reverse gear) (0)\n",
    "     2. Neutral, dont do anything (1)\n",
    "     3. Accelerate to the right (dive forwards) (2)\n",
    "    \n",
    "We can simply render the environment for a few iterations and take a look at the observation. <br />\n",
    "Note how the velocity turns negative when the car engine runs out of power and starts moving backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:-0.5241595303062327, Velocity: 0.0010117940721016405\n",
      "Position:-0.5221435305758053, Velocity: 0.002015999730427466\n",
      "Position:-0.519138445148376, Velocity: 0.003005085427429294\n",
      "Position:-0.5151668112010472, Velocity: 0.003971633947328827\n",
      "Position:-0.5102584102425836, Velocity: 0.004908400958463705\n",
      "Position:-0.5044500353175455, Velocity: 0.005808374925038015\n",
      "Position:-0.49778519696745754, Velocity: 0.006664838350087958\n",
      "Position:-0.49031376700789714, Velocity: 0.007471429959560409\n",
      "Position:-0.4820915599624285, Velocity: 0.008222207045468662\n",
      "Position:-0.47317985316275185, Velocity: 0.008911706799676644\n",
      "Position:-0.4636448480363125, Velocity: 0.009535005126439356\n",
      "Position:-0.45355707687858193, Velocity: 0.01008777115773058\n",
      "Position:-0.44299076133616544, Velocity: 0.01056631554241647\n",
      "Position:-0.4320231307738413, Velocity: 0.010967630562324143\n",
      "Position:-0.42073371051574726, Velocity: 0.011289420258094067\n",
      "Position:-0.4092035914905175, Velocity: 0.011530119025229754\n",
      "Position:-0.3975146939385561, Velocity: 0.01168889755196141\n",
      "Position:-0.38574903845020553, Velocity: 0.01176565548835055\n",
      "Position:-0.3739880376274849, Velocity: 0.011761000822720653\n",
      "Position:-0.3623118210751768, Velocity: 0.01167621655230813\n",
      "Position:-0.35079860525216733, Velocity: 0.011513215823009479\n",
      "Position:-0.3395241180182499, Velocity: 0.011274487233917453\n",
      "Position:-0.3285610855994469, Velocity: 0.010963032418802983\n",
      "Position:-0.31797878729720097, Velocity: 0.010582298302245932\n",
      "Position:-0.30784268072837945, Velocity: 0.010136106568821548\n",
      "Position:-0.29821409784967245, Velocity: 0.009628582878706993\n",
      "Position:-0.289150009626435, Velocity: 0.009064088223237475\n",
      "Position:-0.2807028550666981, Velocity: 0.008447154559736875\n",
      "Position:-0.2729204285435874, Velocity: 0.007782426523110725\n",
      "Position:-0.26584581793171846, Velocity: 0.007074610611868941\n",
      "Position:-0.2595173851139011, Velocity: 0.0063284328178173435\n",
      "Position:-0.25396877987578204, Velocity: 0.005548605238119069\n",
      "Position:-0.24922897807819147, Velocity: 0.004739801797590578\n",
      "Position:-0.24532233524405406, Velocity: 0.003906642834137416\n",
      "Position:-0.24226864727250913, Velocity: 0.0030536879715449133\n",
      "Position:-0.2400832108459722, Velocity: 0.00218543642653692\n",
      "Position:-0.23877687717405668, Velocity: 0.0013063336719155141\n",
      "Position:-0.23835609397161958, Velocity: 0.00042078320243711534\n",
      "Position:-0.23882293195077917, Velocity: -0.0004668379791595847\n",
      "Position:-0.24017509357721656, Velocity: -0.001352161626437404\n",
      "Position:-0.24240590336209106, Velocity: -0.002230809784874486\n",
      "Position:-0.2455042804978743, Velocity: -0.0030983771357832213\n",
      "Position:-0.24945469616567062, Velocity: -0.003950415667796325\n",
      "Position:-0.25423711930844156, Velocity: -0.0047824231427709175\n",
      "Position:-0.2598269560414848, Velocity: -0.005589836733043258\n",
      "Position:-0.26619498911694045, Velocity: -0.00636803307545563\n",
      "Position:-0.2733073249268567, Velocity: -0.00711233580991627\n",
      "Position:-0.2811253563693347, Velocity: -0.00781803144247801\n",
      "Position:-0.28960575046223425, Velocity: -0.008480394092899529\n",
      "Position:-0.2987004698176277, Velocity: -0.009094719355393454\n"
     ]
    }
   ],
   "source": [
    "env.seed(42)  # to make sure that we all have the same initial state\n",
    "observation = env.reset()  # reset all internal values\n",
    "for _ in range(50):\n",
    "    env.render()  # display the current state\n",
    "    action = 2  # lets only accelerate forward\n",
    "    observation, reward, done, info = env.step(action) # perform the random action on the current state of the environment\n",
    "    print(f\"Position:{observation[0]}, Velocity: {observation[1]}\")  # Take a look at the observations\n",
    "    time.sleep(0.1)  # slow down the game a bit\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the first task n this notebook is to fill in the following *chose_action* function, which gets the observation as an argument and returns a suitable action such that the car is able to reach the top of the mountain. \n",
    "\n",
    "The defined if, elif construct may act as a starting point but is not able to reach the top of the mountain.<br />\n",
    "Try if you can change/expand it in such a way that you can reach the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_action(observation):\n",
    "    position, velocity = observation\n",
    "    \n",
    "    if -0.1 < position < 0.1:  # if you current position falls in this intervall chose action 2 (drive forward)\n",
    "        action = 2\n",
    "    \n",
    "    elif velocity < 0 and position < -0.2:  # if your velocity is negative and your position is smaller than -0.2 chose action 0 (drive backwards)\n",
    "        action = 0\n",
    "        \n",
    "    else:  # else do nothing\n",
    "        action = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "observation = env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    action = chose_action(observation)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    time.sleep(0.001)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is one possible solution**\n",
    "\n",
    "This function acts as your first agent in your Reinforcement Learning journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_action_solution(observation):\n",
    "    position, velocity = observation\n",
    "    \n",
    "    if -0.1 < position < 0.4:\n",
    "        action = 2\n",
    "    \n",
    "    elif velocity < 0 and position < -0.2:\n",
    "        action = 0\n",
    "        \n",
    "    else:\n",
    "        action = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You got -1.0 points!\n"
     ]
    }
   ],
   "source": [
    "env.seed(42)\n",
    "observation = env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    action = chose_action_solution(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(f\"You got {reward} points!\")\n",
    "        break\n",
    "    time.sleep(0.001)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next task will be to figure out if you can top the car at the flag (and not overshoot).\n",
    "Try if you can find such an *chose_action* function.\n",
    "\n",
    "Ps: Dont try too hard, it might be harder than you think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_action2(observation):\n",
    "    position, velocity = observation\n",
    "    \n",
    "    action = 0\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = chose_action2(observation)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    time.sleep(0.001)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you can find our approach.<br />\n",
    "Do you see how odd the numbers look and how many elifs are needed to be able to (more or less) fulfill the task?** \n",
    "\n",
    "Now imagine that your possible observation and action spaces would contain hundreds of observations/actions and not only two or three.<br />\n",
    "This task would be daunting.\n",
    "And the worst part is: If you switch tne value in the env.seed() function, your solution might not work at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_action2_solution(observation,):\n",
    "    position, velocity = observation\n",
    "    \n",
    "    \n",
    "    if 0.0 < position < 0.4:\n",
    "        action = 1\n",
    "    elif (position >= 5.03341452e-01 and velocity <= 4.07475660e-04) and \\\n",
    "    (position <= 5.10780594e-01 and velocity >= -2.51391396e-04):\n",
    "        action = 2\n",
    "    elif 5.420594e-01 < position:\n",
    "        action = 0\n",
    "    elif  0.5 < position < 0.505:\n",
    "        action = 2\n",
    "    elif position >= 0.4 and position < 0.41:\n",
    "        action=2\n",
    "    elif 0.49 < position < 0.496:\n",
    "        action = 0\n",
    "    elif position < 0.00938 and velocity > -0.0000001 and not velocity > 0.0472:\n",
    "        action = 2\n",
    "    elif position > -0.5 and velocity > 0.4:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = chose_action2_solution(observation)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    time.sleep(0.001)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But dont worry! This is the point where Reinforcement learning comes to the help and saves us from wasting hours of worktime trying to figure out some good *chose_action* functions.\n",
    "This was the first and last time that you need to implement the agent on your own :)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
