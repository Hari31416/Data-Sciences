{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6S2HVAkSt0p"
      },
      "source": [
        "# Week 2 Assignment: CIFAR-10 Autoencoder\n",
        "\n",
        "For this week, you will create a convolutional autoencoder for the [CIFAR10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset. You are free to choose the architecture of your autoencoder provided that the output image has the same dimensions as the input image.\n",
        "\n",
        "After training, your model should meet loss and accuracy requirements when evaluated with the test dataset. You will then download the model and upload it in the classroom for grading. \n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4iPr2jyisR"
      },
      "source": [
        "***Important:*** *This colab notebook has read-only access so you won't be able to save your changes. If you want to save your work periodically, please click `File -> Save a Copy in Drive` to create a copy in your account, then work from there.*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1mzy2J8_nc1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPuX3RKxl8l5"
      },
      "outputs": [],
      "source": [
        "# # Install packages for compatibility with the autograder\n",
        "# !pip install tensorflow==2.6.0 --quiet\n",
        "# !pip install keras==2.6.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3EXwoz-KHtWO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Gs6Lyc_pd0"
      },
      "source": [
        "## Load and prepare the dataset\n",
        "\n",
        "The [CIFAR 10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset already has train and test splits and you can use those in this exercise. Here are the general steps:\n",
        "\n",
        "* Load the train/test split from TFDS. Set `as_supervised` to `True` so it will be convenient to use the preprocessing function we provided.\n",
        "* Normalize the pixel values to the range [0,1], then return `image, image` pairs for training instead of `image, label`. This is because you will check if the output image is successfully regenerated after going through your autoencoder.\n",
        "* Shuffle and batch the train set. Batch the test set (no need to shuffle).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t9F7YsCNIKSA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/hari31416/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "625df4592e1140c0af356af9eeb89dff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bfcf6b4d4764afb9f3b37d900c050e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d81fdba03e034eb3a78d82e46b3ec35a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m SHUFFLE_BUFFER_SIZE \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m### START CODE HERE (Replace instances of `None` with your code) ###\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# use tfds.load() to fetch the 'train' split of CIFAR-10\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_dataset \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mcifar10\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, as_supervised\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     18\u001b[0m \u001b[39m# preprocess the dataset with the `map_image()` function above\u001b[39;00m\n\u001b[1;32m     19\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mmap(map_image)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/load.py:617\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m    616\u001b[0m   download_and_prepare_kwargs \u001b[39m=\u001b[39m download_and_prepare_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 617\u001b[0m   dbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs)\n\u001b[1;32m    619\u001b[0m \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m   as_dataset_kwargs \u001b[39m=\u001b[39m {}\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:640\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    638\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[1;32m    639\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    641\u001b[0m       dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    642\u001b[0m       download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    643\u001b[0m   )\n\u001b[1;32m    645\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    646\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    647\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    648\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:1448\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1447\u001b[0m   optional_pipeline_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1448\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(  \u001b[39m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1449\u001b[0m     dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_pipeline_kwargs\n\u001b[1;32m   1450\u001b[0m )\n\u001b[1;32m   1451\u001b[0m \u001b[39m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m \u001b[39m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \u001b[39m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m split_generators \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1455\u001b[0m     split_generators\u001b[39m=\u001b[39msplit_generators,\n\u001b[1;32m   1456\u001b[0m     generator_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_examples,\n\u001b[1;32m   1457\u001b[0m     is_beam\u001b[39m=\u001b[39m\u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1458\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/image_classification/cifar.py:83\u001b[0m, in \u001b[0;36mCifar10._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[1;32m     82\u001b[0m   \u001b[39m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m   cifar_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cifar_info\u001b[39m.\u001b[39;49murl)\n\u001b[1;32m     84\u001b[0m   cifar_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cifar_info\n\u001b[1;32m     86\u001b[0m   cifar_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cifar_path, cifar_info\u001b[39m.\u001b[39mprefix)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:686\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_downloader\u001b[39m.\u001b[39mtqdm():\n\u001b[1;32m    685\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extractor\u001b[39m.\u001b[39mtqdm():\n\u001b[0;32m--> 686\u001b[0m     \u001b[39mreturn\u001b[39;00m _map_promise(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_extract, url_or_urls)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:829\u001b[0m, in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    826\u001b[0m all_promises \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    827\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    828\u001b[0m )  \u001b[39m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m res \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    830\u001b[0m     \u001b[39mlambda\u001b[39;49;00m p: p\u001b[39m.\u001b[39;49mget(), all_promises\n\u001b[1;32m    831\u001b[0m )  \u001b[39m# Wait promises\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structures[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[39m0\u001b[39m], other, check_types\u001b[39m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(structures[\u001b[39m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(flatten, structures))])\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structures[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[39m0\u001b[39m], other, check_types\u001b[39m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(structures[\u001b[39m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[39m*\u001b[39;49margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(flatten, structures))])\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:830\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    826\u001b[0m all_promises \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    827\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    828\u001b[0m )  \u001b[39m# Apply the function\u001b[39;00m\n\u001b[1;32m    829\u001b[0m res \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m--> 830\u001b[0m     \u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39;49mget(), all_promises\n\u001b[1;32m    831\u001b[0m )  \u001b[39m# Wait promises\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/promise/promise.py:511\u001b[0m, in \u001b[0;36mPromise.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m     \u001b[39m# type: (Optional[float]) -> T\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target()\n\u001b[0;32m--> 511\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout \u001b[39mor\u001b[39;49;00m DEFAULT_TIMEOUT)\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_settled_value(_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/promise/promise.py:506\u001b[0m, in \u001b[0;36mPromise._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    505\u001b[0m     \u001b[39m# type: (Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m, timeout)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/promise/promise.py:502\u001b[0m, in \u001b[0;36mPromise.wait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    500\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mcls\u001b[39m, promise, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m     \u001b[39m# type: (Promise, Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     async_instance\u001b[39m.\u001b[39;49mwait(promise, timeout)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/promise/async_.py:117\u001b[0m, in \u001b[0;36mAsync.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m promise\u001b[39m.\u001b[39mis_pending:\n\u001b[1;32m    114\u001b[0m         \u001b[39m# We return if the promise is already\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[39m# fulfilled or rejected\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m target\u001b[39m.\u001b[39;49mscheduler\u001b[39m.\u001b[39;49mwait(target, timeout)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/site-packages/promise/schedulers/immediate.py:25\u001b[0m, in \u001b[0;36mImmediateScheduler.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     e\u001b[39m.\u001b[39mset()\n\u001b[1;32m     24\u001b[0m promise\u001b[39m.\u001b[39m_then(on_resolve_or_reject, on_resolve_or_reject)\n\u001b[0;32m---> 25\u001b[0m waited \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m waited:\n\u001b[1;32m     27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTimeout\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m~/anaconda3/envs/data-science/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# preprocessing function\n",
        "def map_image(image, label):\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "\n",
        "  return image, image # dataset label is not used. replaced with the same image input.\n",
        "\n",
        "# parameters\n",
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 1024\n",
        "\n",
        "\n",
        "### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "\n",
        "# use tfds.load() to fetch the 'train' split of CIFAR-10\n",
        "train_dataset = tfds.load('cifar10', split='train', as_supervised=True)\n",
        "\n",
        "# preprocess the dataset with the `map_image()` function above\n",
        "train_dataset = train_dataset.map(map_image)\n",
        "\n",
        "# shuffle and batch the dataset\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# use tfds.load() to fetch the 'test' split of CIFAR-10\n",
        "test_dataset = tfds.load('cifar10', split='test', as_supervised=True)\n",
        "\n",
        "# preprocess the dataset with the `map_image()` function above\n",
        "test_dataset = test_dataset.map(map_image)\n",
        "\n",
        "# batch the dataset\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyOgGJs_t98"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Create the autoencoder model. As shown in the lectures, you will want to downsample the image in the encoder layers then upsample it in the decoder path. Note that the output layer should be the same dimensions as the original image. Your input images will have the shape `(32, 32, 3)`. If you deviate from this, your model may not be recognized by the grader and may fail. \n",
        "\n",
        "We included a few hints to use the Sequential API below but feel free to remove it and use the Functional API just like in the ungraded labs if you're more comfortable with it. Another reason to use the latter is if you want to visualize the encoder output. As shown in the ungraded labs, it will be easier to indicate multiple outputs with the Functional API. That is not required for this assignment though so you can just stack layers sequentially if you want a simpler solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wr-Bok3lRgA3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 16, 16, 256)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 128)       295040    \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 32, 32, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 64)        73792     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 3)         1731      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 741,379\n",
            "Trainable params: 741,379\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-29 12:06:15.794556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# suggested layers to use. feel free to add or remove as you see fit.\n",
        "from keras.layers import Conv2D, UpSampling2D, MaxPool2D\n",
        "\n",
        "# use the Sequential API (you can remove if you want to use the Functional API)\n",
        "model = Sequential()\n",
        "\n",
        "### START CODE HERE ###\n",
        "# use `model.add()` to add layers (if using the Sequential API)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPool2D((2, 2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRWTAijKEVUC"
      },
      "source": [
        "## Configure training parameters\n",
        "\n",
        "We have already provided the optimizer, metrics, and loss in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHIeD9eDETSk"
      },
      "outputs": [],
      "source": [
        "# Please do not change the model.compile() parameters\n",
        "model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQPhm1W_8dC"
      },
      "source": [
        "## Training\n",
        "\n",
        "You can now use [model.fit()](https://keras.io/api/models/model_training_apis/#fit-method) to train your model. You will pass in the `train_dataset` and you are free to configure the other parameters. As with any training, you should see the loss generally going down and the accuracy going up with each epoch. If not, please revisit the previous sections to find possible bugs.\n",
        "\n",
        "*Note: If you get a `dataset length is infinite` error. Please check how you defined `train_dataset`. You might have included a [method that repeats the dataset indefinitely](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMBimOnsRvg0"
      },
      "outputs": [],
      "source": [
        "# parameters (feel free to change this)\n",
        "train_steps = len(train_dataset) // BATCH_SIZE \n",
        "val_steps = len(test_dataset) // BATCH_SIZE\n",
        "\n",
        "### START CODE HERE ###\n",
        "model.fit(train_dataset, epochs=10, steps_per_epoch=train_steps, validation_data=test_dataset, validation_steps=val_steps)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT2l1c-SAaF4"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "You can use this code to test your model locally before uploading to the grader. To pass, your model needs to satisfy these two requirements:\n",
        "\n",
        "* loss must be less than 0.01 \n",
        "* accuracy must be greater than 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFncgqahSQhA"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(test_dataset, steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di6VOHGwIsVM"
      },
      "source": [
        "If you did some visualization like in the ungraded labs, then you might see something like the gallery below. This part is not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpI4skkIA5L"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=12Fy-guiP-3tTPfc9IV2nHhqLvs7LwRo6\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaRSkQPNAPT0"
      },
      "source": [
        "## Save your model\n",
        "\n",
        "Once you are satisfied with the results, you can now save your model. Please download it from the Files window on the left and go back to the Submission portal in Coursera for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLFpLP-c7rDR"
      },
      "outputs": [],
      "source": [
        "model.save('mymodel.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QArMiXJTDxDe"
      },
      "source": [
        "**Congratulations on completing this week's assignment!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
