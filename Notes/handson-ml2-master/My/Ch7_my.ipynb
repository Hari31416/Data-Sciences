{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aggregate\n",
    "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
    "often get better predictions than with the best individual predictor. A group of pre‚Äê\n",
    "dictors is called an *ensemble*; thus, this technique is called *Ensemble Learning*, and an\n",
    "Ensemble Learning algorithm is called an *Ensemble method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A very simple way to create an even better classifier is to aggregate the predictions of\n",
    "each classifier and predict the class that gets the most votes. This majority-vote classifier is called a *hard voting classifier*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3> Hard voting classifier predictions</h3></figcaption>\n",
    "<img src = \"img/07_01.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‚Äê\n",
    "ing it does only slightly better than random guessing), the ensemble can still be a\n",
    "strong learner (achieving high accuracy), provided there are a sufficient number of\n",
    "weak learners and they are sufficiently diverse. How does this work? <br>\n",
    "Suppose you build an ensemble containing 1,000 classifiers that are individ‚Äê\n",
    "ually correct only 51% of the time (barely better than random guessing). If you pre‚Äê\n",
    "dict the majority voted class, you can hope for up to 75% accuracy! However, this is\n",
    "only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
    "which is clearly not the case since they are trained on the same data. They are likely to\n",
    "make the same types of errors, so there will be many majority votes for the wrong\n",
    "class, reducing the ensemble‚Äôs accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Ensemble methods work best when the predictors are as independ‚Äê\n",
    "ent from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble‚Äôs accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple ensemble of two three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)\n",
    "#I had to decrease the test_size to 0.5 otherwise, every model was getting an accuracy of 1 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf,), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see each classifier‚Äôs predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.92\n",
      "RandomForestClassifier 0.9333333333333333\n",
      "SVC 0.92\n",
      "VotingClassifier 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As we can see, the `VotingClassifier` classifier has indeed the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), then you can tell Scikit-Learn to predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. This is called **soft\n",
    "voting**. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes. All you need to do is replace voting=\"hard\" with\n",
    "voting=\"soft\" and ensure that all classifiers can estimate class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9333333333333333\n",
      "RandomForestClassifier 0.9466666666666667\n",
      "VotingClassifier 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "#We have to remove the svm_clf since it does not predict probabilities\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf,),],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, voting_clf): \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> HMM! A tie bewtween `RandomForestClassifier` and `VotingClassifier` !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from choosing defferent classifiers, Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set. When\n",
    "sampling is performed *with replacement*, this method is called **bagging** (short for\n",
    "bootstrap aggregating). When sampling is performed *without replacement*, it is called\n",
    "**pasting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble can make a prediction for a new\n",
    "instance by simply aggregating the predictions of all predictors. The aggregation\n",
    "function is typically the statistical **mode** for classification, or the **average** for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Predictors can all be trained in parallel, via different\n",
    "CPU cores or even different servers. Similarly, predictions can be made in parallel.\n",
    "This is one of the reasons why bagging and pasting are such popular methods: they\n",
    "scale very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging and Pasting in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn offers a simple API for both bagging and pasting with the `BaggingClassifier` class (or `BaggingRegressor` for regression). The `n_jobs` param‚Äê\n",
    "eter tells Scikit-Learn the number of CPU cores to use for training and predictions\n",
    "(‚Äì1 tells Scikit-Learn to use all available cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Baggining Classifier needs more data, so i'm giving in more traing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "max_samples=100, bootstrap=True, n_jobs=-1) #bootstrap=True means that the samples are drawn with replacement\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The `BaggingClassifier` automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class proba‚Äê\n",
    "bilities (i.e., if it has a `predict_proba()` method), which is the case\n",
    "with Decision Trees classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default a `BaggingClassifier` samples m\n",
    "training instances with replacement (`bootstrap=True`), where m is the size of the\n",
    "training set. This means that only about 63% of the training instances are sampled on\n",
    "average for each predictor. The remaining 37% of the training instances that are not\n",
    "sampled are called out-of-bag (oob) instances.<br>\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set. You can evaluate the\n",
    "ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "In Scikit-Learn, you can set `oob_score=True` when creating a `BaggingClassifier` to\n",
    "request an automatic oob evaluation after training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(bag_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should give 94% accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, Great!üòÖ It gives 98% accuracy on test set (Of course, because the test set is very small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.86842105, 0.13157895],\n",
       "       [0.        , 0.01169591, 0.98830409],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.62686567, 0.37313433],\n",
       "       [0.        , 0.01123596, 0.98876404],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.88020833, 0.11979167],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.2893401 , 0.7106599 ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BaggingClassifier` class supports sampling the features as well. This is con‚Äê\n",
    "trolled by two hyperparameters: `max_features` and `bootstrap_features`. They work\n",
    "the same way as `max_samples` and `bootstrap`, but for feature sampling instead of\n",
    "instance sampling. Thus, each predictor will be trained on a random subset of the\n",
    "input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling both training instances *and* features is called the **Random Patches method**. Keeping all training instances (i.e., `bootstrap`=False and `max_samples`=1.0) but sampling features (i.e., `bootstrap_features`=True and/or `max_features` smaller than 1.0) is called the **Random Subspaces method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples`\n",
    "set to the size of the training set. Instead of building a `BaggingClassifier` and pass‚Äê\n",
    "ing it a `DecisionTreeClassifier`, you can instead use the `RandomForestClassifier`\n",
    "class, which is more convenient and optimized for Decision Trees (similarly, there is\n",
    "a `RandomForestRegressor` class for regression tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">With a few exceptions, a `RandomForestClassifier` has all the hyperparameters of a\n",
    "`DecisionTreeClassifier` (to control how trees are grown), plus all the hyperpara‚Äê\n",
    "meters of a `BaggingClassifier` to control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm introduces extra randomness when growing trees;\n",
    "instead of searching for the very best feature when splitting a node, it\n",
    "searches for the best feature among a random subset of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following `BaggingClassifier` is\n",
    "roughly equivalent to the previous `RandomForestClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While growing trees in a `RandomForestClassifier` it is possible to make\n",
    "trees even more *random* by also using random thresholds for each feature rather than\n",
    "searching for the best possible thresholds. A forest of such extremely random trees is simply called an **Extremely Randomized Trees** ensemble (or **Extra-Trees** for short). This trades more bias for a\n",
    "lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">One can create an `Extra-Trees` classifier using Scikit-Learn‚Äôs `ExtraTreesClassifier`\n",
    "class. Its API is identical to the `RandomForestClassifier` class. Similarly, the `ExtraTreesRegressor` class has the same API as the `RandomForestRegressor` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">It is hard to tell in advance whether a `RandomForestClassifier`\n",
    "will perform better or worse than an `ExtraTreesClassifier`. Gen‚Äê\n",
    "erally, the only way to know is to try both and compare them using\n",
    "cross-validation (and tuning the hyperparameters using grid\n",
    "search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training `RandomForest`, Scikit-Learn measures a feature‚Äôs importance by\n",
    "looking at how much the tree nodes that use that feature reduce impurity on average\n",
    "(across all trees in the forest). More precisely, it is a weighted average, where each\n",
    "node‚Äôs weight is equal to the number of training samples that are associated with it.\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it\n",
    "scales the results so that the sum of all importances is equal to 1. You can access the\n",
    "result using the `feature_importances_` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11095775, 0.02625148, 0.46391246, 0.3988783 ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner. *The general idea of most\n",
    "boosting methods is to train predictors sequentially, each trying to correct its prede‚Äê\n",
    "cessor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted. This results in new predic‚Äê\n",
    "tors focusing more and more on the hard cases. This is the technique used by **AdaBoost**.<br>\n",
    "For example, to build an AdaBoost classifier, a first base classifier (such as a `DecisionTree`) is trained and used to make predictions on the training set. The relative weight\n",
    "of misclassified training instances is then increased. A second classifier is trained\n",
    "using the updated weights and again it makes predictions on the training set, weights\n",
    "are updated, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3>Decision boundaries of consecutive predictors</h3></figcaption>\n",
    "<img src = \"img/07_02.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This sequential learning technique has some\n",
    "similarities with *Gradient Descent*, except that instead of tweaking a single predictor‚Äôs\n",
    "parameters to minimize a cost function, `AdaBoost` adds predictors to the ensemble,\n",
    "gradually making it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble makes predictions very much like bag‚Äê\n",
    "ging or pasting, except that predictors have different weights depending on their\n",
    "overall accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is one important drawback to this sequential learning techni‚Äê\n",
    "que: it cannot be parallelized (or only partially), since each predic‚Äê\n",
    "tor can only be trained after the previous predictor has been\n",
    "trained and evaluated. As a result, it does not scale as well as bag‚Äê\n",
    "ging or pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Does it Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance weight $w_(i)$ is initially\n",
    "set to $\\frac{1}{m}$. A first predictor is trained and its weighted error rate $r_1$ is computed on the\n",
    "training set;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3>Weighted error rate of the jth predictor</h3></figcaption>\n",
    "<img src = \"img/07_03.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor‚Äôs weight $\\alpha_j$ is then computed using, \n",
    "$$\\alpha_j = \\eta \\log \\left( \\frac{1-r_j}{r_j} \\right)$$\n",
    "where $Œ∑$ is the learning rate hyperparameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the instance weights are updated using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3> Weight Update Rule</h3></figcaption>\n",
    "<img src = \"img/07_04.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The misclassified instancesare boosted. Then all the instance weights are normalized (i.e., divided by $\\sum_{i=1}^mw^{(i)}$ ).\n",
    "Finally, a new predictor is trained using the updated weights, and the whole process is\n",
    "repeated (the new predictor‚Äôs weight is computed, the instance weights are updated,\n",
    "then another predictor is trained, and so on). The algorithm stops when the desired\n",
    "number of predictors is reached, or when a perfect predictor is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, AdaBoost simply computes the predictions of all the predictors\n",
    "and weighs them using the predictor weights Œ±j. The predicted class is the one that\n",
    "receives the majority of weighted votes \n",
    "$$\\hat{y} = \\arg\\max_{y} \\sum_{j=1}^N \\alpha_j \\cdot y_j$$\n",
    "N = Number of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains an AdaBoost classifier based on 200 Decision Stumps using\n",
    "Scikit-Learn‚Äôs `AdaBoostClassifier` class (there is also an `AdaBoostRegressor` class). **A Decision Stump** is a Decision Tree with `max_depth=1 `‚Äîin\n",
    "other words, a tree composed of a single decision node plus two leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, **this method tries to fit the new predictor to the residual errors made by the previous predictor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Does it Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs go through a simple regression example using Decision Trees as the base predic‚Äê\n",
    "tors. First, let‚Äôs\n",
    "fit a `DecisionTreeRegressor` to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.linspace(-4.5, 5, 400)\n",
    "noise = np.random.normal(0, 1.5, X.shape)\n",
    "y = X**2 + noise\n",
    "X = X.reshape(-1, 1) # make the dimension of X suitable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a second `DecisionTreeRegressor` on the residual errors made by the first\n",
    "predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a third regressor on the residual errors made by the second predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new\n",
    "instance simply by adding up the predictions of all the trees:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3> Gradient Boosting</h3></figcaption>\n",
    "<img src = \"img/07_05.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to train GBRT ensembles is to use Scikit-Learn‚Äôs `GradientBoostingRegressor` class. Much like the `RandomForestRegressor` class, it has hyperparameters to\n",
    "control the growth of Decision Trees (e.g., `max_depth`, `min_samples_leaf`, and so on),\n",
    "as well as hyperparameters to control the ensemble training, such as the number of\n",
    "trees (`n_estimators`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `learning_rate` hyperparameter scales the contribution of each tree. If you set it\n",
    "to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‚Äê\n",
    "ing set, but the predictions will usually generalize better. This is a regularization tech‚Äê\n",
    "nique called **shrinkage**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3>GBRT ensembles with not enough predictors (left) and too many (right)</h3></figcaption>\n",
    "<img src = \"img/07_06.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to use the `staged_predict()` method: it\n",
    "returns an iterator over the predictions made by the ensemble at each stage of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=45)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "    for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even though we started with `n_estimator = 120`, using early stopping, we find that the optimal number of estimators is 45.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(errors, 'o-', alpha=0.4)\n",
    "optimal = np.argmin(errors)\n",
    "optimal_erro = errors[optimal]\n",
    "plt.scatter(optimal, optimal_erro, marker='o', color='r', s=150, label='Minimum')\n",
    "title = \"Validation errors for different number of boosting iterations\"\n",
    "plt.xlabel('Number of tress')\n",
    "plt.title(title)\n",
    "plt.legend()\n",
    "plt.ylim([2, 3.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early\n",
    "(instead of training a large number of trees first and then looking back to find the\n",
    "optimal number). You can do so by setting `warm_start=True`, which makes ScikitLearn keep existing trees when the `fit()` method is called, allowing incremental\n",
    "training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "    if error_going_up == 5:\n",
    "        break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GradientBoostingRegressor` class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instan‚Äê\n",
    "ces, selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. This technique is called\n",
    "*Stochastic Gradient Boosting*.<br>\n",
    "It is also possible to use Gradient Boosting with other cost functions.\n",
    "This is controlled by the `loss` hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XGBoost` is an optimized implementation of Gradient Boosting. It is available\n",
    "in the popular python library `XGBoost`, which stands for *Extreme Gradient Boosting*.\n",
    "This is extremely fast, scalable\n",
    "and portable. In fact, `XGBoost` is often an important component of the winning\n",
    "entries in ML competitions. `XGBoost‚Äôs` API is quite similar to Scikit-Learn‚Äôs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XGBoost` also offers several nice features, such as automatically taking care of early\n",
    "stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:7.67425\n",
      "[1]\tvalidation_0-rmse:5.58704\n",
      "[2]\tvalidation_0-rmse:4.08722\n",
      "[3]\tvalidation_0-rmse:3.07933\n",
      "[4]\tvalidation_0-rmse:2.45200\n",
      "[5]\tvalidation_0-rmse:2.07275\n",
      "[6]\tvalidation_0-rmse:1.85389\n",
      "[7]\tvalidation_0-rmse:1.75117\n",
      "[8]\tvalidation_0-rmse:1.71977\n",
      "[9]\tvalidation_0-rmse:1.70463\n",
      "[10]\tvalidation_0-rmse:1.71308\n",
      "[11]\tvalidation_0-rmse:1.71465\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train,\n",
    "eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is based on a simple idea: instead of using trivial functions\n",
    "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n",
    "why don‚Äôt we train a model to perform this aggregation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows such an\n",
    "ensemble performing a regression task on a new instance. Each of the bottom three\n",
    "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n",
    "(called a blender, or a meta learner) takes these predictions as inputs and makes the\n",
    "final prediction (3.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3>Aggregating predictions using a blending predictor</h3></figcaption>\n",
    "<img src = \"img/07_07.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the blender, a common approach is to use a hold-out set. First, the training set is split in two subsets. The first subset is used to train the\n",
    "predictors in the first layer. Next, the first layer predictors are used to make predictions on the second (held-out)\n",
    "set. This ensures that the predictions are ‚Äúclean,‚Äù since the predictors\n",
    "never saw these instances during training. Now for each instance in the hold-out set there are three predicted values. We can create a new training set using these predicted values as input features (which makes this new training set three-dimensional),\n",
    "and keeping the target values. The blender is trained on this new training set, so it\n",
    "learns to predict the target value given the first layer‚Äôs predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3>Training the blender</h3></figcaption>\n",
    "<img src = \"img/07_08.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually possible to train several different blenders this way (e.g., one using Lin‚Äê\n",
    "ear Regression, another using Random Forest Regression, and so on). Once the training is done, we can make\n",
    "a prediction for a new instance by going through each layer sequentially, as shown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption><h3>Predictions in a multilayer stacking ensemble</h3></figcaption>\n",
    "<img src = \"img/07_09.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Scikit-Learn does not support stacking directly. We can use <a href=\"https://github.com/Menelau/DESlib\">DESlib</a> though."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce44b17c45080b8f56a19c9450d52461d624c968fcd959bb1916985c5ffa2b94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
