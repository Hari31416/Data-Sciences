{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are a powerful way to match patterns in text. These are nothing but trings with a special syntax and llow us to match paerns in other strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/01_01.png \"Common regex patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/01_02.png \"Common Ranges and Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python's `re` Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python comes with a module called `re` that provides regular expression support. Some of the most common methods included in the module are:\n",
    "*`search`: searches for a pattern in a string\n",
    "*`match`: searches for a pattern at the beginning of a string\n",
    "*`findall`: searches for all instances of a pattern in a string\n",
    "*`sub`: replaces all instances of a pattern in a string\n",
    "*`split`: splits a string into a list of substrings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a process of Turning a string or document into tokens (smaller chunks). This is the first step in NLP. This can be done by different methods. However, the most common methods are:\n",
    "* Breaking out words or sentences\n",
    "* Separating punctuation\n",
    "* Separating all hashtags in a tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `nltk` Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` is a Python module that provides a variety of natural language processing (NLP) tools. Some of the most common methods included in the module are:\n",
    "* `word_tokenize`: tokenizes a string into a list of words\n",
    "* `sent_tokenize`: tokenizes a string into a list of sentences\n",
    "* `regexp_tokenize` : tokenize a string or document based on aregular expression pattern\n",
    "* `TweetTokenizer`: special class just for tweet tokenization,\n",
    "allowing you to separate hashtags, mentions and lots ofexclamation points!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "SCENE 1: [wind] [clop clop clop] \n",
    "KING ARTHUR: Whoa there!  [clop clop clop] \n",
    "SOLDIER #1: Halt!  Who goes there?\n",
    "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
    "SOLDIER #1: Pull the other one!\n",
    "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
    "SOLDIER #1: What?  Ridden on a horse?\n",
    "ARTHUR: Yes!\n",
    "SOLDIER #1: You're using coconuts!\n",
    "ARTHUR: What?\n",
    "SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\n",
    "ARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\n",
    "SOLDIER #1: Where'd you get the coconuts?\n",
    "ARTHUR: We found them.\n",
    "SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\n",
    "ARTHUR: What do you mean?\n",
    "SOLDIER #1: Well, this is a temperate zone.\n",
    "ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\n",
    "SOLDIER #1: Are you suggesting coconuts migrate?\n",
    "ARTHUR: Not at all.  They could be carried.\n",
    "SOLDIER #1: What?  A swallow carrying a coconut?\n",
    "ARTHUR: It could grip it by the husk!\n",
    "SOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\n",
    "ARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\n",
    "SOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\n",
    "ARTHUR: Please!\n",
    "SOLDIER #1: Am I right?\n",
    "ARTHUR: I'm not interested!\n",
    "SOLDIER #2: It could be carried by an African swallow!\n",
    "SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\n",
    "SOLDIER #2: Oh, yeah, I agree with that.\n",
    "ARTHUR: Will you ask your master if he wants to join my court at Camelot?!\n",
    "SOLDIER #1: But then of course a-- African swallows are non-migratory.\n",
    "SOLDIER #2: Oh, yeah...\n",
    "SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \n",
    "SOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\n",
    "SOLDIER #1: No, they'd have to have it on a line.\n",
    "SOLDIER #2: Well, simple!  They'd just use a strand of creeper!\n",
    "SOLDIER #1: What, held under the dorsal guiding feathers?\n",
    "SOLDIER #2: Well, why not?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'held', 'needs', \"'s\", 'sun', 'suggesting', 'goes', 'question', 'court', 'ratios', 'Where', 'wants', 'swallows', 'these', 'must', 'England', 'bangin', 'search', \"'d\", 'Britons', 'length', 'You', 'halves', 'ARTHUR', 'Ridden', 'Please', 'got', 'do', '!', 'Pull', 'King', 'of', 'them', 'African', 'use', 'who', 'The', 'temperate', '1', 'velocity', 'martin', 'maybe', 'carrying', 'Yes', 'then', 'grips', 'What', 'matter', 'grip', 'does', 'They', 'pound', ':', \"n't\", '.', 'climes', 'air-speed', 'its', 'from', 'We', 'strand', 'he', 'wind', 'times', 'back', 'point', 'two', 'the', 'SCENE', 'Mercea', 'with', \"'re\", 'it', 'seek', 'land', 'interested', 'Camelot', 'Well', 'one', 'and', 'mean', 'defeator', 'coconut', 'that', 'every', 'creeper', 'Patsy', 'It', 'am', 'minute', 'ridden', 'castle', 'tell', 'Found', 'go', 'will', 'winter', 'through', 'beat', 'agree', 'breadth', '#', 'Who', 'servant', 'other', 'using', 'this', 'get', 'snows', 'under', 'simple', 'Will', 'a', 'join', 'may', 'SOLDIER', 'A', 'found', 'Am', 'wings', 'together', 'could', 'Pendragon', 'Whoa', 'zone', 'swallow', \"'m\", 'But', 'Wait', 'be', 'on', 'or', 'lord', 'Supposing', \"'em\", 'European', 'there', 'my', 'yet', 'feathers', 'covered', 'carry', 'Court', 'master', 'to', 'forty-three', 'bird', 'husk', 'at', '2', '...', 'you', 'our', 'me', 'carried', 'ask', 'line', 'yeah', 'warmer', 'they', ']', 'That', 'weight', 'Saxons', 'since', 'all', 'strangers', 'knights', 'anyway', 'speak', 'ounce', 'Uther', 'Not', '?', 'by', '--', 'five', 'In', 'son', 'maintain', 'kingdom', \"'\", 'dorsal', 'Listen', 'order', 'non-migratory', 'I', 'Arthur', 'fly', 'not', 'an', 'course', 'horse', 'trusty', 'plover', 'where', \"'ve\", 'house', 'migrate', 'are', 'second', '[', 'is', ',', 'just', 'south', 'sovereign', 'empty', 'right', 'KING', 'in', 'coconuts', 'have', 'Halt', 'So', 'here', 'Oh', 'but', 'guiding', 'clop', 'if', 'why', 'tropical', 'No', 'your', 'Are', 'bring'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(text))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` provides a `regexp_tokenize` method that can be used to tokenize a string based on a regular expression pattern.\n",
    "Simply use `regexp_tokenize(string, pattern) `with `string` and `pattern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "pattern = r\"(\\w+|#\\d|\\?|!)\"\n",
    "regexp_tokenize(my_string, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is a frequently used source for NLP text and tasks. The `regexp.tokenize.TweetTokenizer` class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Unlike the syntax for the regex library, with `regexp_tokenize()` you pass the pattern as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n",
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "pattern1 = r\"#\\w+\"\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  '#nlp',\n",
       "  'exercise',\n",
       "  'ive',\n",
       "  'found',\n",
       "  'online',\n",
       "  '!',\n",
       "  '#python'],\n",
       " ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'],\n",
       " ['Thanks', '@datacamp', ':)', '#nlp', '#python']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(text = t) for t in tweets]\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting word length with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Split the script into lines: lines\n",
    "lines = text.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce44b17c45080b8f56a19c9450d52461d624c968fcd959bb1916985c5ffa2b94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
